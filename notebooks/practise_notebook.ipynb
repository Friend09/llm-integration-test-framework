{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRACTISE - INTEGRATION TESTING CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import tempfile\n",
    "import logging\n",
    "from typing import Dict, List, Any, Optional, Tuple\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "import subprocess\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "from openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\"\n",
    ")\n",
    "logger = logging.getLogger(\"llm_integration_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA STRUCTURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class RepoInfo:\n",
    "    \"\"\"dataclass to store repo information\"\"\"\n",
    "\n",
    "    url: str\n",
    "    local_path: str\n",
    "    files: List[Dict[str, Any]]\n",
    "    languages: List[str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class IntegrationPoint:\n",
    "    \"\"\"class to store integration point information.\"\"\"\n",
    "\n",
    "    source: str\n",
    "    target: str\n",
    "    type: str\n",
    "    complexity: int\n",
    "    description: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Component:\n",
    "    \"\"\"data class to store component information\"\"\"\n",
    "\n",
    "    name: str\n",
    "    path: str\n",
    "    language: str\n",
    "    importance: int\n",
    "    dependencies: List[str]\n",
    "    integration_points: List[IntegrationPoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## INTEGRATION FRAMEWORK CLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMIntegrationTestFramework:\n",
    "    \"\"\"\n",
    "    main class for the framework.\n",
    "\n",
    "    this framework analyzes github repos to identify critical integration points and generates a comprehensive testing strategy report.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, openai_api_key: Optional[str] = None):\n",
    "        self.openai_api_key = openai_api_key or os.getenv(\"OPENAI_API_KEY\")\n",
    "        if not self.openai_api_key:  # FALSE IS TRUE,\n",
    "            raise ValueError(\n",
    "                \"OpenAI API key is required. Set OPENAI_API_KEY environment variable.\"\n",
    "            )\n",
    "\n",
    "        self.client = OpenAI(api_key=self.openai_api_key)\n",
    "\n",
    "    def clone_repository(self, repo_url: str) -> str:\n",
    "        logger.info(f\"cloning repository: {repo_url}\")\n",
    "\n",
    "        temp_dir = tempfile.mkdtemp()\n",
    "\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"git\", \"clone\", repo_url, temp_dir],\n",
    "                check=True,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "            )\n",
    "            logger.info(f\"repository cloned to {temp_dir}\")\n",
    "            return temp_dir\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            logger.error(f\"failed to clone repository: {e.stderr}\")\n",
    "            raise RuntimeError(f\"Failed to clone repository: {e.stderr}\")\n",
    "\n",
    "    def scan_repository(self, repo_path: str) -> RepoInfo:\n",
    "        \"\"\"\n",
    "        Scan a repository to extract file information.\n",
    "\n",
    "        Args:\n",
    "            repo_path: Path to the repository.\n",
    "\n",
    "        Returns:\n",
    "            RepoInfo object containing repository information.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Scanning repository: {repo_path}\")\n",
    "\n",
    "        repo_url = self._get_repo_url(repo_path)\n",
    "        files = []\n",
    "        languages = set()\n",
    "\n",
    "        # Walk through the repository\n",
    "        for root, _, filenames in os.walk(repo_path):\n",
    "            for filename in filenames:\n",
    "                # Skip hidden files and directories\n",
    "                if filename.startswith(\".\") or \"/.git/\" in root:\n",
    "                    continue\n",
    "\n",
    "                file_path = Path(root) / filename\n",
    "                relative_path = file_path.relative_to(repo_path)\n",
    "\n",
    "                # Try to detect language based on file extension\n",
    "                ext = file_path.suffix.lower()\n",
    "                language = self._detect_language(ext)\n",
    "                if language:\n",
    "                    languages.add(language)\n",
    "\n",
    "                # Only include relevant files\n",
    "                if language or ext in [\".json\", \".yaml\", \".yml\", \".xml\", \".md\"]:\n",
    "                    try:\n",
    "                        # Read file content (limit to first 1000 lines to avoid memory issues)\n",
    "                        with open(\n",
    "                            file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\"\n",
    "                        ) as f:\n",
    "                            content = \"\".join(f.readlines()[:1000])\n",
    "\n",
    "                        files.append(\n",
    "                            {\n",
    "                                \"path\": str(relative_path),\n",
    "                                \"language\": language,\n",
    "                                \"content\": (\n",
    "                                    content\n",
    "                                    if len(content) < 50000\n",
    "                                    else f\"{content[:25000]}... [content truncated] ...{content[-25000:]}\"\n",
    "                                ),\n",
    "                                \"size\": file_path.stat().st_size,\n",
    "                            }\n",
    "                        )\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Error reading file {file_path}: {str(e)}\")\n",
    "\n",
    "        return RepoInfo(\n",
    "            url=repo_url, local_path=repo_path, files=files, languages=list(languages)\n",
    "        )\n",
    "\n",
    "    def _get_repo_url(self, repo_path: str) -> str:\n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                [\"git\", \"-C\", repo_path, \"config\", \"--get\", \"remote.origin.url\"],\n",
    "                check=True,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "            )\n",
    "            return result.stdout.strip()\n",
    "        except subprocess.CalledProcessError:\n",
    "            return \"Unknown\"\n",
    "\n",
    "    def _detect_language(self, extension: str) -> Optional[str]:\n",
    "        language_map = {\n",
    "            \".py\": \"Python\",\n",
    "            \".js\": \"JavaScript\",\n",
    "            \".ts\": \"TypeScript\",\n",
    "            \".jsx\": \"JavaScript\",\n",
    "            \".tsx\": \"TypeScript\",\n",
    "            \".java\": \"Java\",\n",
    "            \".c\": \"C\",\n",
    "            \".cpp\": \"C++\",\n",
    "            \".h\": \"C/C++\",\n",
    "            \".cs\": \"C#\",\n",
    "            \".go\": \"Go\",\n",
    "            \".rb\": \"Ruby\",\n",
    "            \".php\": \"PHP\",\n",
    "            \".swift\": \"Swift\",\n",
    "            \".kt\": \"Kotlin\",\n",
    "            \".rs\": \"Rust\",\n",
    "            \".scala\": \"Scala\",\n",
    "            \".html\": \"HTML\",\n",
    "            \".css\": \"CSS\",\n",
    "            \".sql\": \"SQL\",\n",
    "        }\n",
    "        return language_map.get(extension)\n",
    "\n",
    "    def analyze_repository(self, repo_info: RepoInfo) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Analyze a repository to identify components and integration points.\n",
    "\n",
    "        Args:\n",
    "            repo_info: Repository information.\n",
    "\n",
    "        Returns:\n",
    "            Dictionary containing analysis results.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Analyzing repository: {repo_info.url}\")\n",
    "\n",
    "        # Prepare the prompt for OpenAI\n",
    "        prompt = self._create_analysis_prompt(repo_info)\n",
    "\n",
    "        # Call OpenAI API\n",
    "        try:\n",
    "            response = self.client.chat.completions.create(\n",
    "                model=\"gpt-4o-mini\",\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are an expert software architect specializing in integration testing. You analyze code repositories to identify critical components, integration points, and recommend testing strategies.\",\n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                temperature=0.2,\n",
    "            )\n",
    "\n",
    "            analysis_text = response.choices[0].message.content\n",
    "\n",
    "            # Try to extract JSON from the response\n",
    "            try:\n",
    "                # Look for JSON block in the response\n",
    "                json_start = analysis_text.find(\"```json\")\n",
    "                json_end = analysis_text.rfind(\"```\")\n",
    "\n",
    "                if json_start != -1 and json_end != -1:\n",
    "                    json_text = analysis_text[json_start + 7 : json_end].strip()\n",
    "                    analysis_result = json.loads(json_text)\n",
    "                else:\n",
    "                    # Try to parse the entire response as JSON\n",
    "                    analysis_result = json.loads(analysis_text)\n",
    "\n",
    "                return analysis_result\n",
    "            except json.JSONDecodeError:\n",
    "                logger.warning(\n",
    "                    \"Failed to parse JSON from OpenAI response, returning raw text\"\n",
    "                )\n",
    "                return {\"raw_analysis\": analysis_text}\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error calling OpenAI API: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _create_analysis_prompt(self, repo_info: RepoInfo) -> str:\n",
    "        \"\"\"Create a prompt for the OpenAI API to analyze the repository.\"\"\"\n",
    "        # Create a summary of the repository\n",
    "        file_count = len(repo_info.files)\n",
    "        language_summary = \", \".join(repo_info.languages)\n",
    "\n",
    "        # Build a list of files to include in the prompt\n",
    "        included_files = []\n",
    "        total_content_length = 0\n",
    "        max_content_length = 100000  # Limit to avoid exceeding OpenAI's token limit\n",
    "\n",
    "        for file in repo_info.files:\n",
    "            # Skip very large files\n",
    "            if file[\"size\"] > 100000:\n",
    "                continue\n",
    "\n",
    "            # Add file content until we reach the maximum\n",
    "            content_length = len(file[\"content\"])\n",
    "            if total_content_length + content_length <= max_content_length:\n",
    "                included_files.append(file)\n",
    "                total_content_length += content_length\n",
    "            else:\n",
    "                # Just add the file path without content\n",
    "                included_files.append(\n",
    "                    {\n",
    "                        \"path\": file[\"path\"],\n",
    "                        \"language\": file[\"language\"],\n",
    "                        \"content\": \"[Content omitted due to size constraints]\",\n",
    "                        \"size\": file[\"size\"],\n",
    "                    }\n",
    "                )\n",
    "\n",
    "        # Create the prompt\n",
    "        prompt = f\"\"\"\n",
    "        # Repository Analysis Request\n",
    "\n",
    "        Analyze the following GitHub repository to identify integration testing needs:\n",
    "\n",
    "        - Repository URL: {repo_info.url}\n",
    "        - Languages: {language_summary}\n",
    "        - File count: {file_count}\n",
    "\n",
    "        ## Repository Structure\n",
    "\n",
    "        I'll provide a selection of file contents below. Please analyze these to identify:\n",
    "\n",
    "        1. Critical components and their dependencies\n",
    "        2. Integration points between components\n",
    "        3. Recommended integration testing approaches\n",
    "        4. Test prioritization based on component criticality\n",
    "        5. Specific test strategy recommendations\n",
    "\n",
    "        For each integration point, assess:\n",
    "        - Type (API, database, service-to-service, etc.)\n",
    "        - Complexity (1-5 scale, where 5 is most complex)\n",
    "        - Testing approach recommendations\n",
    "\n",
    "        ## Files\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        for file in included_files:\n",
    "            prompt += f\"\"\"\n",
    "        ### {file['path']} ({file['language'] or 'Unknown'})\n",
    "\n",
    "        ```\n",
    "        {file['content']}\n",
    "        ```\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        prompt += \"\"\"\n",
    "        ## Response Format\n",
    "\n",
    "        Please provide your analysis in JSON format with the following structure:\n",
    "\n",
    "        ```json\n",
    "        {\n",
    "          \"components\": [\n",
    "            {\n",
    "              \"name\": \"string\",\n",
    "              \"path\": \"string\",\n",
    "              \"language\": \"string\",\n",
    "              \"description\": \"string\",\n",
    "              \"dependencies\": [\"string\"],\n",
    "              \"importance\": 1-5\n",
    "            }\n",
    "          ],\n",
    "          \"integration_points\": [\n",
    "            {\n",
    "              \"source\": \"string\",\n",
    "              \"target\": \"string\",\n",
    "              \"type\": \"string\",\n",
    "              \"complexity\": 1-5,\n",
    "              \"description\": \"string\",\n",
    "              \"testing_approach\": \"string\"\n",
    "            }\n",
    "          ],\n",
    "          \"testing_strategy\": {\n",
    "            \"recommended_approach\": \"string\",\n",
    "            \"justification\": \"string\",\n",
    "            \"test_order\": [\"string\"],\n",
    "            \"critical_areas\": [\"string\"]\n",
    "          },\n",
    "          \"recommendations\": [\n",
    "            {\n",
    "              \"description\": \"string\",\n",
    "              \"priority\": \"string\",\n",
    "              \"effort\": \"string\"\n",
    "            }\n",
    "          ]\n",
    "        }\n",
    "        ```\n",
    "\n",
    "        Focus on providing actionable insights for integration testing.\n",
    "        \"\"\"\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    # start from here\n",
    "    def generate_report():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST FRAMEWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 11:24:52,755 - llm_integration_test - INFO - cloning repository: https://github.com/Friend09/llm-smoke-test-framework\n",
      "2025-04-28 11:24:54,222 - llm_integration_test - INFO - repository cloned to /var/folders/43/572h075x24q9rq1slmdfw9yw0000gn/T/tmp_l1f8lel\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/var/folders/43/572h075x24q9rq1slmdfw9yw0000gn/T/tmp_l1f8lel'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "framework = LLMIntegrationTestFramework()\n",
    "framework.clone_repository(\n",
    "    repo_url=\"https://github.com/Friend09/llm-smoke-test-framework\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 11:26:59,366 - llm_integration_test - INFO - Scanning repository: /var/folders/43/572h075x24q9rq1slmdfw9yw0000gn/T/tmp_l1f8lel\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Python']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_info = framework.scan_repository(\n",
    "    repo_path=\"/var/folders/43/572h075x24q9rq1slmdfw9yw0000gn/T/tmp_l1f8lel\"\n",
    ")\n",
    "\n",
    "repo_info.languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 11:27:56,009 - llm_integration_test - INFO - Analyzing repository: https://github.com/Friend09/llm-smoke-test-framework\n",
      "2025-04-28 11:28:16,307 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'components': [{'name': 'Config',\n",
       "   'path': 'config/config.py',\n",
       "   'language': 'Python',\n",
       "   'description': 'Configuration class for the LLM Smoke Test Framework, managing environment variables and settings.',\n",
       "   'dependencies': [],\n",
       "   'importance': 5},\n",
       "  {'name': 'WebCrawler',\n",
       "   'path': 'core/crawler.py',\n",
       "   'language': 'Python',\n",
       "   'description': 'Responsible for crawling web pages and extracting data, including screenshots.',\n",
       "   'dependencies': ['Config'],\n",
       "   'importance': 5},\n",
       "  {'name': 'LLMAnalyzer',\n",
       "   'path': 'core/llm_analyzer.py',\n",
       "   'language': 'Python',\n",
       "   'description': 'Analyzes web pages using OpenAI models, including both text and vision capabilities.',\n",
       "   'dependencies': ['Config'],\n",
       "   'importance': 5},\n",
       "  {'name': 'TestGenerator',\n",
       "   'path': 'core/test_generator.py',\n",
       "   'language': 'Python',\n",
       "   'description': 'Generates test scripts based on the analysis of web pages.',\n",
       "   'dependencies': ['Config', 'LLMAnalyzer'],\n",
       "   'importance': 5},\n",
       "  {'name': 'SitemapCrawler',\n",
       "   'path': 'core/sitemap_crawler.py',\n",
       "   'language': 'Python',\n",
       "   'description': 'Crawls and maps the structure of websites for testing.',\n",
       "   'dependencies': ['Config'],\n",
       "   'importance': 4},\n",
       "  {'name': 'SitemapLoader',\n",
       "   'path': 'core/sitemap_loader.py',\n",
       "   'language': 'Python',\n",
       "   'description': 'Loads and filters URLs from pre-generated sitemap files.',\n",
       "   'dependencies': ['Config'],\n",
       "   'importance': 3}],\n",
       " 'integration_points': [{'source': 'WebCrawler',\n",
       "   'target': 'LLMAnalyzer',\n",
       "   'type': 'Service-to-Service',\n",
       "   'complexity': 3,\n",
       "   'description': 'WebCrawler extracts page data and passes it to LLMAnalyzer for analysis.',\n",
       "   'testing_approach': 'Mock WebCrawler to provide controlled page data for LLMAnalyzer tests.'},\n",
       "  {'source': 'LLMAnalyzer',\n",
       "   'target': 'TestGenerator',\n",
       "   'type': 'Service-to-Service',\n",
       "   'complexity': 4,\n",
       "   'description': 'LLMAnalyzer generates analysis results that are consumed by TestGenerator to create test scripts.',\n",
       "   'testing_approach': 'Use integration tests to ensure TestGenerator correctly interprets analysis from LLMAnalyzer.'},\n",
       "  {'source': 'SitemapCrawler',\n",
       "   'target': 'WebCrawler',\n",
       "   'type': 'Service-to-Service',\n",
       "   'complexity': 3,\n",
       "   'description': 'SitemapCrawler discovers URLs that WebCrawler will later crawl.',\n",
       "   'testing_approach': 'Test the integration by verifying that URLs discovered by SitemapCrawler are correctly processed by WebCrawler.'},\n",
       "  {'source': 'Config',\n",
       "   'target': 'All Components',\n",
       "   'type': 'Configuration Management',\n",
       "   'complexity': 2,\n",
       "   'description': 'All components depend on the configuration settings provided by the Config class.',\n",
       "   'testing_approach': 'Unit tests for Config validation and integration tests to ensure all components read configuration correctly.'}],\n",
       " 'testing_strategy': {'recommended_approach': 'Behavior-Driven Development (BDD) with Cucumber',\n",
       "  'justification': 'The framework generates Cucumber test scripts, making BDD a natural fit for integration testing.',\n",
       "  'test_order': ['Unit tests for individual components',\n",
       "   'Integration tests for service interactions',\n",
       "   'End-to-end tests for complete workflows'],\n",
       "  'critical_areas': ['Data extraction accuracy from WebCrawler',\n",
       "   'Correct analysis generation by LLMAnalyzer',\n",
       "   'Test script generation fidelity by TestGenerator']},\n",
       " 'recommendations': [{'description': 'Implement comprehensive unit tests for each core component.',\n",
       "   'priority': 'High',\n",
       "   'effort': 'Medium'},\n",
       "  {'description': 'Develop integration tests focusing on the interactions between WebCrawler, LLMAnalyzer, and TestGenerator.',\n",
       "   'priority': 'High',\n",
       "   'effort': 'High'},\n",
       "  {'description': 'Create end-to-end tests that simulate user interactions and validate the entire workflow from crawling to test generation.',\n",
       "   'priority': 'Medium',\n",
       "   'effort': 'High'},\n",
       "  {'description': 'Ensure configuration validation tests are in place to catch misconfigurations early.',\n",
       "   'priority': 'Medium',\n",
       "   'effort': 'Low'}]}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_analysis = framework.analyze_repository(repo_info=repo_info)\n",
    "repo_analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
